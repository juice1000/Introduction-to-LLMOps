{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5752523",
   "metadata": {},
   "source": [
    "# LLMOps Evaluation Playground\n",
    "\n",
    "This notebook demonstrates how to evaluate the LLMOps chatbot system using various metrics and test cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d6df3a",
   "metadata": {},
   "source": [
    "## 1. Setup Environment\n",
    "\n",
    "Import required libraries and set up the evaluation environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce28dfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# Import project modules\n",
    "from app.config import Config\n",
    "from app.chains import ChatChain\n",
    "from eval.ragas_eval import RAGASEvaluator\n",
    "from eval.promptfoo import PromptFooEvaluator\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b02e77",
   "metadata": {},
   "source": [
    "## 2. Initialize System Components\n",
    "\n",
    "Load configuration and initialize the chatbot system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6acb885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config = Config()\n",
    "\n",
    "# Initialize chat chain\n",
    "chat_chain = ChatChain(config)\n",
    "\n",
    "# Initialize evaluators\n",
    "ragas_eval = RAGASEvaluator(config)\n",
    "promptfoo_eval = PromptFooEvaluator()\n",
    "\n",
    "print(\"System components initialized!\")\n",
    "print(f\"OpenAI API Key configured: {'Yes' if config.OPENAI_API_KEY else 'No'}\")\n",
    "print(f\"Vector store path: {config.VECTOR_STORE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e248cde9",
   "metadata": {},
   "source": [
    "## 3. Load Test Data\n",
    "\n",
    "Load the evaluation test samples and examine the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c075f9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test samples\n",
    "test_samples = ragas_eval.load_test_samples()\n",
    "\n",
    "print(f\"Loaded {len(test_samples.get('questions', []))} test questions\")\n",
    "print(\"\\nSample questions:\")\n",
    "for i, question in enumerate(test_samples.get('questions', [])[:3]):\n",
    "    print(f\"{i+1}. {question}\")\n",
    "\n",
    "# Create DataFrame for easier analysis\n",
    "if test_samples['questions']:\n",
    "    test_df = pd.DataFrame({\n",
    "        'question': test_samples['questions'],\n",
    "        'ground_truth': test_samples['ground_truths'],\n",
    "        'context': test_samples.get('contexts', [[]] * len(test_samples['questions']))\n",
    "    })\n",
    "    \n",
    "    display(test_df.head())\n",
    "else:\n",
    "    print(\"No test samples found. Please check the test_samples.json file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c992acea",
   "metadata": {},
   "source": [
    "## 4. Run Chat System Tests\n",
    "\n",
    "Test the chatbot system with our evaluation questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208c5209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "async def run_chat_tests():\n",
    "    \"\"\"Run the chat system on test questions.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i, question in enumerate(test_samples.get('questions', [])):\n",
    "        print(f\"Processing question {i+1}/{len(test_samples['questions'])}: {question[:50]}...\")\n",
    "        \n",
    "        try:\n",
    "            # Test with RAG\n",
    "            result = await chat_chain.process_message(\n",
    "                message=question,\n",
    "                conversation_id=f\"test_{i}\",\n",
    "                use_rag=True\n",
    "            )\n",
    "            \n",
    "            results.append({\n",
    "                'question_id': i,\n",
    "                'question': question,\n",
    "                'answer': result['response'],\n",
    "                'sources': result.get('sources', []),\n",
    "                'conversation_id': result['conversation_id']\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing question {i}: {e}\")\n",
    "            results.append({\n",
    "                'question_id': i,\n",
    "                'question': question,\n",
    "                'answer': f\"Error: {str(e)}\",\n",
    "                'sources': [],\n",
    "                'conversation_id': f\"test_{i}\"\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run the tests\n",
    "if test_samples.get('questions'):\n",
    "    chat_results = await run_chat_tests()\n",
    "    \n",
    "    # Display results\n",
    "    results_df = pd.DataFrame(chat_results)\n",
    "    print(f\"\\nGenerated {len(chat_results)} responses\")\n",
    "    display(results_df[['question', 'answer']].head())\n",
    "else:\n",
    "    print(\"No questions to test. Skipping chat tests.\")\n",
    "    chat_results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d14034",
   "metadata": {},
   "source": [
    "## 5. RAGAS Evaluation\n",
    "\n",
    "Evaluate the chat responses using RAGAS metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff77892a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if chat_results and test_samples.get('questions'):\n",
    "    # Prepare data for RAGAS evaluation\n",
    "    questions = [r['question'] for r in chat_results]\n",
    "    answers = [r['answer'] for r in chat_results]\n",
    "    contexts = [[source] for r in chat_results for source in r.get('sources', ['No context'])]\n",
    "    ground_truths = test_samples['ground_truths']\n",
    "    \n",
    "    # Ensure contexts has the right length\n",
    "    if len(contexts) != len(questions):\n",
    "        contexts = [['No context available']] * len(questions)\n",
    "    \n",
    "    # Run RAGAS evaluation\n",
    "    ragas_results = ragas_eval.evaluate_rag_system(\n",
    "        questions=questions,\n",
    "        answers=answers,\n",
    "        contexts=contexts,\n",
    "        ground_truths=ground_truths\n",
    "    )\n",
    "    \n",
    "    print(\"RAGAS Evaluation Results:\")\n",
    "    for metric, score in ragas_results.items():\n",
    "        print(f\"  {metric}: {score:.3f}\")\n",
    "    \n",
    "    # Generate evaluation report\n",
    "    report = ragas_eval.generate_evaluation_report(ragas_results)\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(report)\n",
    "else:\n",
    "    print(\"No chat results available for RAGAS evaluation.\")\n",
    "    ragas_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fe780d",
   "metadata": {},
   "source": [
    "## 6. Visualize Results\n",
    "\n",
    "Create visualizations of the evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cc64c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ragas_results:\n",
    "    # Create a bar plot of RAGAS metrics\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Metrics bar plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    metrics = list(ragas_results.keys())\n",
    "    scores = list(ragas_results.values())\n",
    "    \n",
    "    bars = plt.bar(metrics, scores, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])\n",
    "    plt.title('RAGAS Evaluation Metrics')\n",
    "    plt.ylabel('Score')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Add score labels on bars\n",
    "    for bar, score in zip(bars, scores):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                f'{score:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # Radar chart\n",
    "    plt.subplot(1, 2, 2, projection='polar')\n",
    "    angles = [i * 2 * 3.14159 / len(metrics) for i in range(len(metrics))]\n",
    "    angles += angles[:1]  # Complete the circle\n",
    "    scores_radar = scores + scores[:1]  # Complete the circle\n",
    "    \n",
    "    plt.plot(angles, scores_radar, 'o-', linewidth=2)\n",
    "    plt.fill(angles, scores_radar, alpha=0.25)\n",
    "    plt.xticks(angles[:-1], metrics)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.title('RAGAS Metrics Radar Chart')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary statistics\n",
    "    avg_score = sum(scores) / len(scores)\n",
    "    print(f\"\\nSummary Statistics:\")\n",
    "    print(f\"Average Score: {avg_score:.3f}\")\n",
    "    print(f\"Best Metric: {metrics[scores.index(max(scores))]} ({max(scores):.3f})\")\n",
    "    print(f\"Worst Metric: {metrics[scores.index(min(scores))]} ({min(scores):.3f})\")\n",
    "else:\n",
    "    print(\"No RAGAS results to visualize.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a3674d",
   "metadata": {},
   "source": [
    "## 7. Response Quality Analysis\n",
    "\n",
    "Analyze the quality and characteristics of generated responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b4ffc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if chat_results:\n",
    "    # Analyze response characteristics\n",
    "    response_stats = []\n",
    "    \n",
    "    for result in chat_results:\n",
    "        answer = result['answer']\n",
    "        response_stats.append({\n",
    "            'length': len(answer),\n",
    "            'word_count': len(answer.split()),\n",
    "            'sentence_count': answer.count('.') + answer.count('!') + answer.count('?'),\n",
    "            'has_sources': len(result.get('sources', [])) > 0,\n",
    "            'source_count': len(result.get('sources', []))\n",
    "        })\n",
    "    \n",
    "    stats_df = pd.DataFrame(response_stats)\n",
    "    \n",
    "    # Visualize response statistics\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Response length distribution\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.hist(stats_df['word_count'], bins=10, edgecolor='black')\n",
    "    plt.title('Response Word Count Distribution')\n",
    "    plt.xlabel('Word Count')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    # Character length distribution\n",
    "    plt.subplot(2, 3, 2)\n",
    "    plt.hist(stats_df['length'], bins=10, edgecolor='black', color='orange')\n",
    "    plt.title('Response Character Length Distribution')\n",
    "    plt.xlabel('Character Count')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    # Source usage\n",
    "    plt.subplot(2, 3, 3)\n",
    "    source_counts = stats_df['source_count'].value_counts().sort_index()\n",
    "    plt.bar(source_counts.index, source_counts.values, color='green')\n",
    "    plt.title('Source Count Distribution')\n",
    "    plt.xlabel('Number of Sources')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    # Summary statistics table\n",
    "    plt.subplot(2, 3, 4)\n",
    "    plt.axis('off')\n",
    "    summary_stats = stats_df.describe()\n",
    "    table_data = []\n",
    "    for col in ['word_count', 'length', 'source_count']:\n",
    "        table_data.append([col, f\"{summary_stats.loc['mean', col]:.1f}\", \n",
    "                          f\"{summary_stats.loc['std', col]:.1f}\"])\n",
    "    \n",
    "    table = plt.table(cellText=table_data, \n",
    "                     colLabels=['Metric', 'Mean', 'Std Dev'],\n",
    "                     cellLoc='center', loc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    plt.title('Response Statistics Summary')\n",
    "    \n",
    "    # Response examples\n",
    "    plt.subplot(2, 3, 5)\n",
    "    plt.axis('off')\n",
    "    plt.text(0.1, 0.9, \"Sample Responses:\", fontsize=14, fontweight='bold', transform=plt.gca().transAxes)\n",
    "    \n",
    "    for i, result in enumerate(chat_results[:2]):\n",
    "        question = result['question'][:40] + \"...\" if len(result['question']) > 40 else result['question']\n",
    "        answer = result['answer'][:60] + \"...\" if len(result['answer']) > 60 else result['answer']\n",
    "        \n",
    "        y_pos = 0.7 - i * 0.3\n",
    "        plt.text(0.1, y_pos, f\"Q: {question}\", fontsize=10, transform=plt.gca().transAxes)\n",
    "        plt.text(0.1, y_pos - 0.1, f\"A: {answer}\", fontsize=9, transform=plt.gca().transAxes, \n",
    "                style='italic', color='blue')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nResponse Analysis Summary:\")\n",
    "    print(f\"Average response length: {stats_df['word_count'].mean():.1f} words\")\n",
    "    print(f\"Responses with sources: {stats_df['has_sources'].sum()}/{len(stats_df)} ({stats_df['has_sources'].mean()*100:.1f}%)\")\n",
    "    print(f\"Average sources per response: {stats_df['source_count'].mean():.1f}\")\n",
    "else:\n",
    "    print(\"No chat results available for analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222f1f3b",
   "metadata": {},
   "source": [
    "## 8. Create Custom Test Cases\n",
    "\n",
    "Create and run custom test cases for specific scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1733cf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom test cases\n",
    "custom_tests = [\n",
    "    {\n",
    "        \"description\": \"Technical question about RAG\",\n",
    "        \"question\": \"How does chunking affect RAG performance?\",\n",
    "        \"expected_keywords\": [\"chunk\", \"performance\", \"retrieval\"],\n",
    "        \"forbidden_keywords\": [\"don't know\", \"cannot\"]\n",
    "    },\n",
    "    {\n",
    "        \"description\": \"Operational question\",\n",
    "        \"question\": \"What are the best practices for monitoring LLM applications?\",\n",
    "        \"expected_keywords\": [\"monitoring\", \"metrics\", \"performance\"],\n",
    "        \"forbidden_keywords\": [\"unsure\", \"unclear\"]\n",
    "    },\n",
    "    {\n",
    "        \"description\": \"Out-of-scope question\",\n",
    "        \"question\": \"What's the weather like today?\",\n",
    "        \"expected_keywords\": [],\n",
    "        \"forbidden_keywords\": [\"sunny\", \"rainy\", \"cloudy\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Run custom tests\n",
    "async def run_custom_tests():\n",
    "    custom_results = []\n",
    "    \n",
    "    for i, test in enumerate(custom_tests):\n",
    "        print(f\"Running custom test {i+1}: {test['description']}\")\n",
    "        \n",
    "        try:\n",
    "            result = await chat_chain.process_message(\n",
    "                message=test['question'],\n",
    "                conversation_id=f\"custom_test_{i}\",\n",
    "                use_rag=True\n",
    "            )\n",
    "            \n",
    "            # Check keywords\n",
    "            answer_lower = result['response'].lower()\n",
    "            expected_found = [kw for kw in test['expected_keywords'] if kw.lower() in answer_lower]\n",
    "            forbidden_found = [kw for kw in test['forbidden_keywords'] if kw.lower() in answer_lower]\n",
    "            \n",
    "            custom_results.append({\n",
    "                'test': test['description'],\n",
    "                'question': test['question'],\n",
    "                'answer': result['response'],\n",
    "                'expected_found': expected_found,\n",
    "                'forbidden_found': forbidden_found,\n",
    "                'sources': result.get('sources', []),\n",
    "                'pass': len(forbidden_found) == 0 and (len(test['expected_keywords']) == 0 or len(expected_found) > 0)\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            custom_results.append({\n",
    "                'test': test['description'],\n",
    "                'question': test['question'],\n",
    "                'answer': f\"Error: {str(e)}\",\n",
    "                'expected_found': [],\n",
    "                'forbidden_found': [],\n",
    "                'sources': [],\n",
    "                'pass': False\n",
    "            })\n",
    "    \n",
    "    return custom_results\n",
    "\n",
    "# Execute custom tests\n",
    "custom_test_results = await run_custom_tests()\n",
    "\n",
    "# Display results\n",
    "print(\"\\nCustom Test Results:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for result in custom_test_results:\n",
    "    status = \"âœ… PASS\" if result['pass'] else \"âŒ FAIL\"\n",
    "    print(f\"\\n{status} - {result['test']}\")\n",
    "    print(f\"Q: {result['question']}\")\n",
    "    print(f\"A: {result['answer'][:100]}...\")\n",
    "    if result['expected_found']:\n",
    "        print(f\"âœ“ Expected keywords found: {result['expected_found']}\")\n",
    "    if result['forbidden_found']:\n",
    "        print(f\"âœ— Forbidden keywords found: {result['forbidden_found']}\")\n",
    "\n",
    "# Summary\n",
    "passed_tests = sum(1 for r in custom_test_results if r['pass'])\n",
    "print(f\"\\nCustom Tests Summary: {passed_tests}/{len(custom_test_results)} passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01947d31",
   "metadata": {},
   "source": [
    "## 9. Export Results\n",
    "\n",
    "Save evaluation results for further analysis and reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d839d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare export data\n",
    "export_data = {\n",
    "    'timestamp': pd.Timestamp.now().isoformat(),\n",
    "    'ragas_metrics': ragas_results if 'ragas_results' in locals() else {},\n",
    "    'test_results': chat_results if 'chat_results' in locals() else [],\n",
    "    'custom_test_results': custom_test_results if 'custom_test_results' in locals() else [],\n",
    "    'response_statistics': response_stats if 'response_stats' in locals() else []\n",
    "}\n",
    "\n",
    "# Save to JSON\n",
    "output_dir = project_root / 'eval' / 'results'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "output_file = output_dir / f'evaluation_results_{pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")}.json'\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(export_data, f, indent=2, default=str)\n",
    "\n",
    "print(f\"âœ… Evaluation results exported to: {output_file}\")\n",
    "\n",
    "# Create summary report\n",
    "summary_report = f\"\"\"\n",
    "LLMOps Evaluation Summary Report\n",
    "Generated: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "ğŸ“Š RAGAS Metrics:\n",
    "{chr(10).join([f'  - {k}: {v:.3f}' for k, v in (ragas_results.items() if 'ragas_results' in locals() and ragas_results else {})])}\n",
    "\n",
    "ğŸ“‹ Test Results:\n",
    "  - Standard tests: {len(chat_results) if 'chat_results' in locals() else 0}\n",
    "  - Custom tests: {len(custom_test_results) if 'custom_test_results' in locals() else 0}\n",
    "  - Custom tests passed: {passed_tests if 'passed_tests' in locals() else 0}\n",
    "\n",
    "ğŸ“ˆ Response Statistics:\n",
    "  - Average word count: {stats_df['word_count'].mean():.1f if 'stats_df' in locals() and not stats_df.empty else 'N/A'}\n",
    "  - Responses with sources: {stats_df['has_sources'].sum() if 'stats_df' in locals() and not stats_df.empty else 0}\n",
    "\n",
    "ğŸ¯ Overall Assessment:\n",
    "{ragas_eval.generate_evaluation_report(ragas_results).split('Performance Assessment:')[1] if 'ragas_results' in locals() and ragas_results else 'No RAGAS evaluation performed'}\n",
    "\"\"\"\n",
    "\n",
    "print(summary_report)\n",
    "\n",
    "# Save summary report\n",
    "report_file = output_dir / f'summary_report_{pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")}.txt'\n",
    "with open(report_file, 'w') as f:\n",
    "    f.write(summary_report)\n",
    "\n",
    "print(f\"\\nğŸ“„ Summary report saved to: {report_file}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
