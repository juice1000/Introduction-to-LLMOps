#!/bin/bash

# Simple LLMOps Setup Script
# This script sets up a basic LLMOps environment with Python 3.12

set -e

echo "ğŸš€ Setting up Simple LLMOps Environment..."

# Check if Python 3.12 is installed
if ! command -v python3.12 &> /dev/null; then
    echo "âŒ Python 3.12 is required but not installed."
    echo "   Install with: brew install python@3.12"
    exit 1
fi

# Create virtual environment if it doesn't exist
if [ ! -d "venv" ]; then
    echo "ğŸ“¦ Creating virtual environment with Python 3.12..."
    python3.12 -m venv venv
fi

# Activate virtual environment
echo "ğŸ”§ Activating virtual environment..."
source venv/bin/activate

# Upgrade pip
echo "â¬†ï¸  Upgrading pip..."
pip install --upgrade pip

# Install dependencies
echo "ğŸ“¥ Installing Python dependencies..."
pip install -r requirements.txt

# Check if .env file exists
if [ ! -f ".env" ]; then
    echo "âš™ï¸ Creating .env file..."
    cat > .env << EOF
# Ollama Configuration
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=gemma3:1b

# Vector Store
CHROMA_PERSIST_DIRECTORY=./data/vector_store

# App Settings
API_TITLE=Simple Insurance Chatbot
API_VERSION=1.0.0
EOF
    echo "âœ… Created .env file with default settings"
fi

# Check if Ollama is installed
if ! command -v ollama &> /dev/null; then
    echo "âŒ Ollama is not installed. Please install Ollama first:"
    echo "   Visit: https://ollama.ai/download"
    echo "   Or run: curl -fsSL https://ollama.ai/install.sh | sh"
    exit 1
fi

# Check if Ollama is running
echo "ğŸ” Checking Ollama service..."
if ! curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
    echo "ğŸš€ Starting Ollama service..."
    ollama serve &
    sleep 3
    
    if ! curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
        echo "âŒ Failed to start Ollama service. Please start it manually:"
        echo "   Run: ollama serve"
        exit 1
    fi
fi

# Pull the required model
echo "ğŸ“¥ Checking for Gemma 3:1B model..."
if ! ollama list | grep -q "gemma3:1b"; then
    echo "ğŸ¤– Pulling Gemma 3:1B model (this may take a few minutes)..."
    ollama pull gemma3:1b
    
    if [ $? -ne 0 ]; then
        echo "âŒ Failed to pull Gemma 3:1B model. Please check your internet connection."
        exit 1
    fi
else
    echo "âœ… Gemma 3:1B model is already available"
fi

# Test the model
echo "ğŸ§ª Testing Ollama model..."
test_response=$(ollama run gemma3:1b "Hello, respond with 'Working'")
if [[ "$test_response" == *"Working"* ]] || [[ "$test_response" == *"working"* ]]; then
    echo "âœ… Ollama model is working correctly"
else
    echo "âš ï¸  Ollama model test uncertain, but continuing setup..."
fi

# Create necessary directories
echo "ğŸ“ Creating directories..."
mkdir -p data/vector_store data/documents

echo ""
echo "âœ… Setup complete!"
echo ""
echo "Next steps:"
echo "1. Add documents to data/documents/"
echo "2. Start the chatbot: uvicorn app.main:app --reload"
echo "3. Visit http://localhost:8000/docs to test the API"
echo ""
echo "ğŸ¯ Simple LLMOps Environment Ready!"
