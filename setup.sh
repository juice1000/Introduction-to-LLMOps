#!/bin/bash

# LLMOps Setup Script
# This script sets up the entire LLMOps environment

set -e

echo "üöÄ Setting up LLMOps Environment..."

# Check if Python is installed
if ! command -v python3 &> /dev/null; then
    echo "‚ùå Python 3 is required but not installed."
    exit 1
fi

# Check if pip is installed
if ! command -v pip &> /dev/null; then
    echo "‚ùå pip is required but not installed."
    exit 1
fi

# Create virtual environment if it doesn't exist
if [ ! -d "venv" ]; then
    echo "üì¶ Creating virtual environment..."
    python3.9 -m venv venv
fi

# Activate virtual environment
echo "üîß Activating virtual environment..."
source venv/bin/activate

# Install dependencies
echo "üì• Installing Python dependencies..."
pip install -r requirements.txt

# Check if .env file exists
if [ ! -f ".env" ]; then
    echo "‚öôÔ∏è Creating .env file from template..."
    cp .env .env.example
    echo "‚ö†Ô∏è  Please edit .env file and configure Ollama settings if needed"
fi

# Check if Ollama is installed
if ! command -v ollama &> /dev/null; then
    echo "‚ùå Ollama is not installed. Please install Ollama first:"
    echo "   Visit: https://ollama.ai/download"
    echo "   Or run: curl -fsSL https://ollama.ai/install.sh | sh"
    exit 1
fi

# Check if Ollama is running
echo "üîç Checking Ollama service..."
if ! curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
    echo "üöÄ Starting Ollama service..."
    ollama serve &
    sleep 5
    
    if ! curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
        echo "‚ùå Failed to start Ollama service. Please start it manually:"
        echo "   Run: ollama serve"
        exit 1
    fi
fi

# Pull the required model
echo "üì• Checking for Gemma 3:12B model..."
if ! ollama list | grep -q "gemma3:12b"; then
    echo "ü§ñ Pulling Gemma 3:12B model (this may take a few minutes)..."
    ollama pull gemma3:12b
    
    if [ $? -ne 0 ]; then
        echo "‚ùå Failed to pull Gemma 3:12B model. Please check your internet connection."
        exit 1
    fi
else
    echo "‚úÖ Gemma 3:12B model is already available"
fi

# Test the model
echo "üß™ Testing Ollama model..."
test_response=$(ollama run gemma3:12b "Hello, please respond with 'Ollama is working'")
if [[ "$test_response" == *"working"* ]]; then
    echo "‚úÖ Ollama model is working correctly"
else
    echo "‚ö†Ô∏è  Ollama model test failed, but continuing setup..."
fi

# Create necessary directories
echo "üìÅ Creating directories..."
mkdir -p data/raw data/processed data/vector_store eval/results

# Run seed data script for insurance
echo "üå± Seeding insurance sample data..."
python scripts/seed_data_insurance.py

# Build vector index
echo "üîç Building vector index..."
python scripts/build_index.py

# Check if PromptFoo is installed
if command -v promptfoo &> /dev/null; then
    echo "‚úÖ PromptFoo is available"
else
    echo "‚ÑπÔ∏è  To install PromptFoo for evaluation: npm install -g promptfoo"
fi

echo ""
echo "‚úÖ Setup complete!"
echo ""
echo "Next steps:"
echo "1. Ollama is running with Gemma 2B model"
echo "2. Start the insurance chatbot: uvicorn app.main:app --reload"
echo "3. Visit http://localhost:8000 to test the API"
echo "4. Use notebooks/eval_playground.ipynb for evaluation"
echo "5. Try asking: 'How do I file an auto insurance claim?'"
echo ""
echo "ÔøΩ Welcome to SafeGuard Insurance AI Assistant!"
